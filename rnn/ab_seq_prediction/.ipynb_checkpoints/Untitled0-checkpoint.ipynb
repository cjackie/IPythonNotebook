{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import tensorflow as tf\n",
      "import numpy as np\n",
      "can_reuse = False\n",
      "inference_mode = False # False: training mode. True: inference mode\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "# topology\n",
      "\n",
      "--------------         ------------\n",
      "|            |         |          |\n",
      "| LSTM cell  |  -----> | linear   | ----> softmax -->\n",
      "|            |         |          |\n",
      "--------------         ------------"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lstm_units = 10\n",
      "batch_size = 100\n",
      "seq_len = 10\n",
      "with tf.variable_scope(\"lstm\")  as lstm_scope:\n",
      "    # Allow the LSTM variables to be reused.\n",
      "    if can_reuse:\n",
      "        lstm_scope.reuse_variables()\n",
      "    can_reuse = True\n",
      "    \n",
      "    cell = tf.nn.rnn_cell.LSTMCell(lstm_units)\n",
      "    cell_state = cell.zero_state(batch_size, tf.float32)\n",
      "    \n",
      "    # inference model\n",
      "    seq_in = tf.placeholder(tf.float32, shape=[batch_size, seq_len])\n",
      "    output, _ = cell(seq_in, cell_state)\n",
      "    \n",
      "    # training model\n",
      "    training_seq_in = tf.placeholder(tf.float32, shape=[batch_size, seq_len, 1])\n",
      "    training_outputs, _ = tf.nn.dynamic_rnn(cell=cell,\n",
      "                        inputs=training_seq_in,\n",
      "                        initial_state=cell_state,\n",
      "                        dtype=tf.float32)\n",
      "    training_output = training_outputs[:,-1,:]\n",
      "training_output, output, cell_state\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 2,
       "text": [
        "(<tf.Tensor 'lstm/strided_slice:0' shape=(100, 10) dtype=float32>,\n",
        " <tf.Tensor 'lstm/LSTMCell/mul_2:0' shape=(100, 10) dtype=float32>,\n",
        " LSTMStateTuple(c=<tf.Tensor 'lstm/zeros:0' shape=(100, 10) dtype=float32>, h=<tf.Tensor 'lstm/zeros_1:0' shape=(100, 10) dtype=float32>))"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "s_size = 2\n",
      "with tf.variable_scope('logits') as logits_scope:\n",
      "    weights = tf.Variable(tf.zeros(shape=[lstm_units, s_size]))\n",
      "    biase = tf.Variable(tf.zeros(shape=[s_size]))\n",
      "    if inference_mode == True:\n",
      "        logits = tf.matmul(output, weights) + biase\n",
      "    else:\n",
      "        logits = tf.matmul(training_output, weights) + biase\n",
      "    \n",
      "with tf.variable_scope('loss') as loss_scope:\n",
      "    labels_in = tf.placeholder(tf.float32, shape= [batch_size, s_size])\n",
      "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, labels_in))\n",
      "    \n",
      "# softmax\n",
      "with tf.variable_scope('softmax') as softmax:\n",
      "    softmax = tf.nn.softmax(logits)\n",
      "    \n",
      "# optimizer\n",
      "optimizer = tf.train.GradientDescentOptimizer(0.1)\n",
      "optimizer = tf.train.AdamOptimizer(0.001)\n",
      "\n",
      "training = optimizer.minimize(loss)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# generate data sequence of ab\n",
      "sequence = []\n",
      "for i in range(batch_size*seq_len):\n",
      "    if i % 2:\n",
      "        sequence.append('a')\n",
      "    else:\n",
      "        sequence.append('b')\n",
      "sequence = map(lambda x: ord(x), sequence)\n",
      "data, labels = [], []\n",
      "for i in range(batch_size):\n",
      "    data.append(map(lambda x: [x], sequence[i:i+seq_len]))\n",
      "    if sequence[i+seq_len] == ord('a'):\n",
      "        labels.append([1,0])\n",
      "    elif sequence[i+seq_len] == ord('b'):\n",
      "        labels.append([0,1])\n",
      "    else:\n",
      "        raise Exception(\"d\")\n",
      "\n",
      "labels = np.array(labels)\n",
      "data = np.array(data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "if not inference_mode:    \n",
      "    steps = 10000\n",
      "    sess = tf.Session()\n",
      "    \n",
      "    sess.run(tf.global_variables_initializer())\n",
      "    for i in range(steps):\n",
      "        sess.run(training, feed_dict={training_seq_in: data, labels_in: labels})\n",
      "#         print(sess.run(loss, feed_dict={training_seq_in: data, labels_in: labels}))\n",
      "#         print(sess.run(softmax, feed_dict={seq_in: data})) \n",
      "        if (sess.run(loss, feed_dict={training_seq_in: data, labels_in: labels}) < 0.01):\n",
      "            break\n",
      "            \n",
      "    print(sess.run(loss, feed_dict={training_seq_in: data, labels_in: labels}))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.00999827\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "weigt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}