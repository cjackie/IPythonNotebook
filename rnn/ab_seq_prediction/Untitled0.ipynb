{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import tensorflow as tf\n",
      "import numpy as np\n",
      "can_reuse = False\n",
      "inference_mode = False # False: training mode. True: inference mode\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "# topology\n",
      "\n",
      "--------------         ------------\n",
      "|            |         |          |\n",
      "| LSTM cell  |  -----> | linear   | ----> softmax -->\n",
      "|            |         |          |\n",
      "--------------         ------------"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lstm_units = 10\n",
      "batch_size = 100\n",
      "seq_len = 10\n",
      "with tf.variable_scope(\"lstm\")  as lstm_scope:\n",
      "    # Allow the LSTM variables to be reused.\n",
      "    if can_reuse:\n",
      "        lstm_scope.reuse_variables()\n",
      "    can_reuse = True\n",
      "    \n",
      "    cell = tf.nn.rnn_cell.LSTMCell(lstm_units)\n",
      "    cell_state = cell.zero_state(batch_size, tf.float32)\n",
      "    \n",
      "    # inference model\n",
      "    char_in = tf.placeholder(tf.float32, shape=[None, 1])\n",
      "    state_c = tf.placeholder(tf.float32, shape=[None, lstm_units])\n",
      "    state_h = tf.placeholder(tf.float32, shape=[None, lstm_units])\n",
      "    output, _ = cell(char_in, (state_c, state_h))\n",
      "    \n",
      "    # training model shape is \n",
      "    training_seq_in = tf.placeholder(tf.float32, shape=[batch_size, seq_len, 1])\n",
      "    training_outputs, _ = tf.nn.dynamic_rnn(cell=cell,\n",
      "                        inputs=training_seq_in,\n",
      "                        initial_state=cell_state,\n",
      "                        dtype=tf.float32)\n",
      "    training_output = training_outputs[:,-1,:]\n",
      "training_output, output, cell_state, cell.state_size\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 2,
       "text": [
        "(<tf.Tensor 'lstm/strided_slice:0' shape=(100, 10) dtype=float32>,\n",
        " <tf.Tensor 'lstm/LSTMCell/mul_2:0' shape=(?, 10) dtype=float32>,\n",
        " LSTMStateTuple(c=<tf.Tensor 'lstm/zeros:0' shape=(100, 10) dtype=float32>, h=<tf.Tensor 'lstm/zeros_1:0' shape=(100, 10) dtype=float32>),\n",
        " LSTMStateTuple(c=10, h=10))"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "s_size = 2\n",
      "with tf.variable_scope('logits') as logits_scope:\n",
      "    weights = tf.Variable(tf.zeros(shape=[lstm_units, s_size]))\n",
      "    biase = tf.Variable(tf.zeros(shape=[s_size]))\n",
      "    inference_logits = tf.matmul(output, weights) + biase\n",
      "    training_logits = tf.matmul(training_output, weights) + biase\n",
      "#     if inference_mode == True:\n",
      "#         logits = tf.matmul(output, weights) + biase\n",
      "#     else:\n",
      "#         logits = tf.matmul(training_output, weights) + biase\n",
      "\n",
      "# softmax for inference\n",
      "with tf.variable_scope('softmax') as softmax:\n",
      "    inference_softmax = tf.nn.softmax(inference_logits)\n",
      "    training_softmax = tf.nn.softmax(training_logits)\n",
      "    \n",
      "# loss\n",
      "with tf.variable_scope('loss') as loss_scope:\n",
      "    labels_in = tf.placeholder(tf.float32, shape= [batch_size, s_size])\n",
      "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(training_logits, labels_in))\n",
      "\n",
      "    \n",
      "# optimizer\n",
      "# optimizer = tf.train.GradientDescentOptimizer(0.1)\n",
      "optimizer = tf.train.AdamOptimizer(0.001)\n",
      "\n",
      "training = optimizer.minimize(loss)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# generate data sequence of ab\n",
      "sequence = []\n",
      "for i in range(batch_size*seq_len):\n",
      "    if i % 2:\n",
      "        sequence.append('a')\n",
      "    else:\n",
      "        sequence.append('b')\n",
      "sequence = map(lambda x: ord(x), sequence)\n",
      "data, labels = [], []\n",
      "for i in range(batch_size):\n",
      "    data.append(map(lambda x: [x], sequence[i:i+seq_len]))\n",
      "    if sequence[i+seq_len] == ord('a'):\n",
      "        labels.append([1,0])\n",
      "    elif sequence[i+seq_len] == ord('b'):\n",
      "        labels.append([0,1])\n",
      "    else:\n",
      "        raise Exception(\"d\")\n",
      "\n",
      "labels = np.array(labels)\n",
      "data = np.array(data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "if not inference_mode:    \n",
      "    steps = 10000\n",
      "    sess = tf.Session()\n",
      "    \n",
      "    sess.run(tf.global_variables_initializer())\n",
      "    for i in range(steps):\n",
      "        sess.run(training, feed_dict={training_seq_in: data, labels_in: labels})\n",
      "#         print(sess.run(loss, feed_dict={training_seq_in: data, labels_in: labels}))\n",
      "#         print(sess.run(softmax, feed_dict={seq_in: data})) \n",
      "        if (sess.run(loss, feed_dict={training_seq_in: data, labels_in: labels}) < 0.01):\n",
      "            break\n",
      "            \n",
      "    print(sess.run(loss, feed_dict={training_seq_in: data, labels_in: labels}))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.0099967\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test_seq_0 = map(lambda s: [ord(s)], ['b','a','b', 'a','b', 'a', 'b', 'a', 'b', 'a'])\n",
      "test_seq_1 = map(lambda s: [ord(s)], ['b','a','b', 'b','b', 'a', 'b', 'b', 'b', 'a'])\n",
      "test_seq_2 = map(lambda s: [ord(s)], ['b','b','b', 'b','b', 'b', 'b', 'b', 'b', 'a'])\n",
      "test_seq_3 = map(lambda s: [ord(s)], ['a','b','a', 'b', 'a', 'b', 'a', 'b', 'a', 'b'])\n",
      "test_data = np.zeros((batch_size, seq_len,1))\n",
      "test_data[0,:,:] = test_seq_0\n",
      "test_data[1,:,:] = test_seq_1\n",
      "test_data[2,:,:] = test_seq_2\n",
      "test_data[3,:,:] = test_seq_3\n",
      "\n",
      "print(sess.run(training_softmax, feed_dict={training_seq_in:test_data})[0:4])\n",
      "\n",
      "# ??? this does not match expectation.\n",
      "with tf.variable_scope(\"lstm\")  as lstm_scope:\n",
      "    lstm_scope.reuse_variables()\n",
      "    # inference mode ,'a','b', 'a','b', 'a', 'b', 'a', 'b', 'a', 'b','b'\n",
      "    test_seq_inf_0 = map(lambda s: ord(s), ['b','a','b', 'b','b', 'a', 'b', 'b', 'b', 'a'])\n",
      "    state_inf = cell.zero_state(1, tf.float32)\n",
      "    for s in test_seq_inf_0[0:-2]:\n",
      "        test_data_inf = [[s]]\n",
      "        test_data_inf = tf.convert_to_tensor(test_data_inf, dtype=tf.float32)\n",
      "        _, state_inf = cell(test_data_inf, state_inf)\n",
      "\n",
      "    feed_dict = {\n",
      "        char_in: [[test_seq_inf_0[-1]]],\n",
      "        state_c: sess.run(state_inf.c),\n",
      "        state_h: sess.run(state_inf.h)\n",
      "    }\n",
      "    print(sess.run(inference_softmax, feed_dict=feed_dict))\n",
      "    \n",
      "\n",
      "    \n",
      "\n",
      "\n",
      "# # inference mode\n",
      "# test_seq_inf_0 = map(lambda s: ord(s), ['b','a','b', 'a','b', 'a', 'b', 'a', 'b', 'a'])\n",
      "# test_seq_inf_1 = map(lambda s: ord(s), ['a','b','a', 'b', 'a', 'b', 'a', 'b', 'a', 'b'])\n",
      "# test_data_inf = np.zeros((batch_size, seq_len))\n",
      "# test_data_inf[0, :] = test_seq_inf_0\n",
      "# test_data_inf[1, :] = test_seq_inf_1\n",
      "# print(sess.run(inference_softmax, feed_dict={seq_in: test_data_inf})[0:2,:])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[ 0.0089828   0.99101722]\n",
        " [ 0.01193474  0.9880653 ]\n",
        " [ 0.19665833  0.80334169]\n",
        " [ 0.98908991  0.01091015]]\n",
        "[[  9.99428689e-01   5.71296085e-04]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 8
    }
   ],
   "metadata": {}
  }
 ]
}